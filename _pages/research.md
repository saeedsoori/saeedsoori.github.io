---
permalink: /research/
title: ""
---

## Machine Learning


We improve the peformance and scalability of machine learning algorithms on parallel high performance and cloud computing platforms.Our approach targets both theoretical aspects and systems engineering. 


* ASYNC ([pdf](http://www.paramathic.com/wp-content/uploads/2019/10/ASYNC.pdf "pdf"), [code](https://github.com/ASYNCframework/ASYNCframework "code")): A novel framework to support asycnhronous optimization on hetergenous platforms.

* DAve-QN ([pdf](http://www.paramathic.com/wp-content/uploads/2019/09/Dave-QN.pdf "pdf"), [code](https://github.com/DAve-QN/source "code")): the first asynchronous quasi-newton method with superlinear convergence.

* EFF-RES ([pdf](http://www.paramathic.com/wp-content/uploads/2019/09/Eff-res.pdf "pdf")): efficient distributed
algorithms for computing effective resistances in undirected networks.

* CA-SFISTA ([pdf](http://www.paramathic.com/wp-content/uploads/2019/09/CA-FISTA.pdf "pdf"), [code](https://github.com/saeedsoori/CA-Methods "code")): communication-avoiding algorithms for high performance computing. 


## Approximate Matrix Algorithms
Kernel methods in statistical learning and scientific computing can cause challenges such as inverting large matrices or solving an high dimensional linear system. We develop efficient frameworks to approximate these computations while satisfying user-specific accuracies.

* MatRox ([pdf](http://www.paramathic.com/wp-content/uploads/2019/10/MatRox.pdf "pdf"), [code](https://github.com/kobeliu85/MatRox_RU "code")): high-performance framework to efficiently compress and approximate matrix computations.


## Quantized Deep Learning

We develop scalable optimization algorithms for training deep neural networks on distributed platforms. We design novel quantizated stochastic algorithms to reduce the communication cost of transferred bits among GPUs in a cluster. We adpot data parallelism and MPI communication to build an effiecient message passing while preserving convergence rates.




